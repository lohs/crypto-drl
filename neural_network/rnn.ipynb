{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "import keras.initializers\n",
    "from keras.layers import Dense, Layer, LSTM, GRU, SimpleRNN, RNN\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# reproducible results\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Globals\n",
    "transaction_cost = .001\n",
    "crypto_params = ['BTC-USD', 'ETH-USD']\n",
    "time_period_params = ['_hr_24', '_hr_1']\n",
    "\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\n",
    "\n",
    "def naive_baseline(y_train):\n",
    "    y_naive = np.roll(y_train, 1)\n",
    "    print(np.mean(keras.losses.mean_squared_error(y_train, y_naive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-21 10:03:52.970602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-21 10:03:53.069774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-21 10:03:53.070124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# force cpu only\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse raw data\n",
    "Turn time series into supervised learning problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_trade(period_return):\n",
    "    if(abs(period_return) <= transaction_cost):\n",
    "        return 0\n",
    "    elif period_return > transaction_cost:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_raw(dataset):\n",
    "    print(dataset)\n",
    "    df = pd.read_csv(f'../data/raw/{dataset}.csv')\n",
    "\n",
    "    # assumption: make decision at open of trading period\n",
    "    # shift these cols\n",
    "    shift_header = ['low', 'high', 'open','close', 'volume']\n",
    "    for col in shift_header:\n",
    "        new_header = f'last_{col}'\n",
    "        df[new_header] = df[col].shift(1)\n",
    "    \n",
    "    # rename current observations\n",
    "    df = df.rename(columns={'open':'current_open', 'close':'target_close'})\n",
    "\n",
    "    # drop low, high, volume cols and rearrange remaining\n",
    "    df = df[['time','last_low', 'last_high', 'last_open','last_close','last_volume', 'current_open', 'target_close']]\n",
    "\n",
    "    # calculate period returns and label trade\n",
    "    df['period_return'] = (df['target_close'].values - df['current_open'].values) / df['current_open'].values\n",
    "    df['target_trade'] = df['period_return'].apply(label_trade)\n",
    "\n",
    "    # assumptions:\n",
    "    # perfect close prediction\n",
    "    # end period in cash\n",
    "    # 0 cost to short sell\n",
    "    df['profit'] = df['period_return'] * df['target_trade'] * df['target_close'] * (1 - transaction_cost)\n",
    "    df['profit'].sum()\n",
    "\n",
    "    # save labeled dataset\n",
    "    df.to_csv(f'../data/labeled_{dataset}.csv', header=True, index=False, mode='w')\n",
    "    print(f'Saved to ../data/labeled_{dataset}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crypto in crypto_params:\n",
    "    for time_period in time_period_params:\n",
    "        dataset = crypto + time_period\n",
    "        label_raw(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BTC-USD_hr_24\"\n",
    "df = pd.read_csv(f'../data/labeled_{dataset}.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index, unit='s')\n",
    "target = 'target_close'\n",
    "\n",
    "#df.isnull().sum()\n",
    "\n",
    "# drop NaN\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity (Augmented Dickey-Fuller Test)\n",
    "It is essential to determine whether the time series is \"stationary\". Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostic tests. Moreover, stationarity is decisive in characterizing the prediction problem and whether to use a more advanced architecture. In particular, we can expect a plain RNN to perform poorly if the data is non-stationary as the RNN exhibits fixed auto-covariance. \n",
    "\n",
    "We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. \n",
    "\n",
    "If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $\\Delta y_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above $(1-\\alpha)$, then we cannot reject that there is a unit root. Note that a subset of the time series is used to reduce the memory requirements of the test. We use the first 200,000 samples to test for stationarity. While the test statistic is sensitive to the data size, the ADF test is always accepted at the 99\\% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF: -0.9935794670602157\n",
      "p-value: 0.7555988337293066,\n",
      "N: 729, \n",
      "critical values: {'1%': -3.4393520240470554, '5%': -2.8655128165959236, '10%': -2.5688855736949163}\n"
     ]
    }
   ],
   "source": [
    "# check stationarity of target\n",
    "# 1 min 56 sec\n",
    "sample = df[target]\n",
    "\n",
    "adf, p, usedlag, n_observations, cvs, aic = sm.tsa.stattools.adfuller(sample)\n",
    "print(f'ADF: {adf}\\np-value: {p},\\nN: {n_observations}, \\ncritical values: {cvs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accept null as p-value > 0.01, can't reject test at 99% confidence level. Series seems to be non-stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "close price is stationary when using differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoregressive Model Identification: The partial auto-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to determine the number of lags, the sequence length, required in the RNN by statistical analysis. A brute-force approach will in general be too time-consuming.\n",
    "\n",
    "A partial auto-correlation at lag $h\\geq 2$ is a conditional auto-correlation between a variable, $X_t$, and its $h^{th}$ lag, $X_{t-h}$ under the assumption that we control for the values of the intermediate lags, $X_{t-1},\\dots, X_{t-h+1}$:\n",
    "\n",
    "$$\\begin{align}\\tau_h&:=\\tau(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})\\\\\n",
    "&:=\\frac{\\gamma(X_t, X_{t-h}; X_{t-1},\\dots, X_{t-h+1})}{\\sqrt{\\gamma(X_t |X_{t-1},\\dots, X_{t-h+1})\\gamma(X_{t-h} |X_{t-1},\\dots, X_{t-h+1}))}},\n",
    "\\end{align}$$\n",
    "where $\\gamma_h:=\\gamma(X_tX_{t-h})$ is the lag-$h$ autocovariance. The partial autocorrelation function $\\tau_h:\\mathbb{N} \\rightarrow [-1,1]$ is a map $h:\\mapsto \\tau_h$.\n",
    "\n",
    "The estimated partial auto-correlation function (PACF) can be used to identify the order of an autoregressive time series model. Values of $|\\tau_h|$ greater or equal to $\\frac{\\Phi^{-1}(\\alpha)}{\\sqrt{T}}$, where $T$ is the number of observations and $\\Phi(z)$ is the standard normal CDF, are significant lag $h$ partial autocorelations at the $\\alpha$ confidence level.\n",
    "\n",
    "We use the stattools package to estimate the PACF. The `nlags` parameter is the maximum number of lags used for PACF estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_features = ['last_low', 'last_high', \"last_open\", \"last_volume\", \"current_open\"] # continuous input\n",
    "use_features = [target]\n",
    "# already defined above\n",
    "#target = ['target_close'] # continuous output\n",
    "n_steps_ahead = 1 # forecasting horizon\n",
    "n_steps = 1\n",
    "n_features = len(use_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_steps set to 1\n"
     ]
    }
   ],
   "source": [
    "pacf = sm.tsa.stattools.pacf(df[use_features], nlags=30)\n",
    "\n",
    "T = len(df[use_features])\n",
    "\n",
    "sig_test = lambda tau_h: np.abs(tau_h) > 2.58/np.sqrt(T)\n",
    "\n",
    "for i in range(len(pacf)):\n",
    "    if sig_test(pacf[i]) == False:\n",
    "        n_steps = i - 1\n",
    "        print('n_steps set to', n_steps)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pacf, label='pacf')\n",
    "plt.plot([2.58/np.sqrt(T)]*30, label='99% confidence interval (upper)')\n",
    "plt.plot([-2.58/np.sqrt(T)]*30, label='99% confidence interval (lower)')\n",
    "plt.xlabel('number of lags')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 1-t% training, t test\n",
    "def partition(df, t):\n",
    "    split_index = int(len(df) * (1-t))\n",
    "\n",
    "    df_train = df.iloc[:split_index]\n",
    "    df_test = df.iloc[split_index:]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_X_train_full, uni_X_test = partition(df, 0.2)\n",
    "# train and validation set\n",
    "uni_X_train, uni_X_valid = partition(uni_X_train_full, 0.2)\n",
    "\n",
    "# reshape 1d for further use\n",
    "uni_X_train_full = uni_X_train_full[[target]]\n",
    "uni_X_train = uni_X_train[[target]]\n",
    "uni_X_test = uni_X_test[[target]]\n",
    "uni_X_valid = uni_X_valid[[target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_weight = 0.8\n",
    "# split = int(len(df) * train_weight)\n",
    "\n",
    "# df_train_full = df[use_features].iloc[:split]\n",
    "# df_test = df[use_features].iloc[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged_features(df, n_steps, n_steps_ahead):\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame of time series to be lagged\n",
    "    n_steps: number of lags, i.e. sequence length\n",
    "    n_steps_ahead: forecasting horizon\n",
    "    \"\"\"\n",
    "    lag_list = []\n",
    "    \n",
    "    for lag in range(n_steps + n_steps_ahead - 1, n_steps_ahead - 1, -1):\n",
    "        lag_list.append(df.shift(lag))\n",
    "    lag_array = np.dstack([i[n_steps+n_steps_ahead-1:] for i in lag_list])\n",
    "    # We swap the last two dimensions so each slice along the first dimension\n",
    "    # is the same shape as the corresponding segment of the input time series \n",
    "    lag_array = np.swapaxes(lag_array, 1, -1)\n",
    "    return lag_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardization\n",
    "Much less affected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale univariate\n",
    "\n",
    "# train_weight = 0.8\n",
    "# split = int(len(df) * train_weight)\n",
    "\n",
    "# uni_X_train_full = df[use_features].iloc[:split]\n",
    "# uni_X_test = df[use_features].iloc[split:]\n",
    "\n",
    "# note that for a multivariate time series, you would need to scale \n",
    "# each variable by its own mean and standard deviation in the training set\n",
    "mean = np.float64(uni_X_train_full.mean())\n",
    "std_dev = np.float64(uni_X_train_full.std())\n",
    "\n",
    "stdize_input = lambda x: (x - mean) / std_dev\n",
    "\n",
    "# standardize by mean and std dev of training set to avoid bias\n",
    "uni_X_train = uni_X_train.apply(stdize_input)\n",
    "uni_X_valid = uni_X_valid.apply(stdize_input)\n",
    "uni_X_test = uni_X_test.apply(stdize_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "x_train = get_lagged_features(uni_X_train, n_steps, n_steps_ahead)\n",
    "y_train =  uni_X_train.values[n_steps + n_steps_ahead - 1:]\n",
    "y_train_timestamps = uni_X_train.index[n_steps + n_steps_ahead - 1:]\n",
    "\n",
    "# validation\n",
    "x_valid = get_lagged_features(uni_X_valid, n_steps, n_steps_ahead)\n",
    "y_valid =  uni_X_valid.values[n_steps + n_steps_ahead - 1:]\n",
    "y_valid_timestamps = uni_X_valid.index[n_steps + n_steps_ahead - 1:]\n",
    "\n",
    "# test\n",
    "x_test = get_lagged_features(uni_X_test, n_steps, n_steps_ahead)\n",
    "y_test =  uni_X_test.values[n_steps + n_steps_ahead - 1:]\n",
    "y_test_timestamps = uni_X_test.index[n_steps + n_steps_ahead - 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler()\n",
    "std_scaler.fit(X_train_full)\n",
    "print(f'Mean: {std_scaler.mean_}, Std Dev: {std_scaler.var_}')\n",
    "\n",
    "std_train = std_scaler.transform(X_train_full)\n",
    "# use training mean and std dev to avoid future bias\n",
    "std_test = std_scaler.transform(X_test)\n",
    "\n",
    "# inverse_std_train = std_scaler.inverse_transform(std_train)\n",
    "# inverse_std_test = std_scaler.inverse_transform(std_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize Scaling (MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scaler = preprocessing.MinMaxScaler()\n",
    "norm_scaler.fit(X_train_full)\n",
    "print(f'Min: {norm_scaler.data_min_}, Max: {norm_scaler.data_max_}')\n",
    "\n",
    "norm_train = norm_scaler.transform(X_train_full)\n",
    "# use training mean and std dev to avoid future bias\n",
    "norm_test = norm_scaler.transform(X_test)\n",
    "\n",
    "# inverse_std_train = norm_scaler.inverse_transform(norm_train)\n",
    "# inverse_std_test = norm_scaler.inverse_transform(norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Forecasting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_baseline(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "lr_model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[n_steps, 1]),\n",
    "keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "lr_model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "history = lr_model.fit(x_train, y_train, epochs=100,\n",
    "                    validation_data=(x_valid, y_valid))\n",
    "lr_model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.axis([1, 20, 0, 0.5])\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "deep_rnn_model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(100, return_sequences=True, input_shape=[n_steps, 1]),\n",
    "    keras.layers.SimpleRNN(100, return_sequences=True),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "deep_rnn_model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "deep_rnn_history = deep_rnn_model.fit(x_train, y_train, epochs=20, validation_data=(x_valid, y_valid))\n",
    "deep_rnn_model.evaluate(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(deep_rnn_history.history[\"loss\"], deep_rnn_history.history[\"val_loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([tensor.shape for tensor in (x_train, y_train, x_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(GRU(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "\n",
    "def LSTM_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True, return_sequences = True))\n",
    "  model.add(LSTM(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), unroll=True)) \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model\n",
    "\n",
    "def SimpleRNN_(n_units = 10, l1_reg=0, seed=0):\n",
    "  model = Sequential()\n",
    "  model.add(SimpleRNN(n_units, activation='tanh', kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), recurrent_initializer=keras.initializers.orthogonal(seed), kernel_regularizer=l1(l1_reg), input_shape=(x_train.shape[1], x_train.shape[-1]), unroll=True, stateful=False))  \n",
    "  model.add(Dense(1, kernel_initializer=keras.initializers.glorot_uniform(seed), bias_initializer=keras.initializers.glorot_uniform(seed), kernel_regularizer=l1(l1_reg)))\n",
    "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=100, min_delta=1e-7, restore_best_weights=True)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'rnn': {\n",
    "        'model' : None, 'function':SimpleRNN_, 'l1_reg': 0.0, 'H': 20,\n",
    "        'color': 'blue', 'label': 'RNN', 'history':[]\n",
    "    },\n",
    "    'gru': {\n",
    "        'model': None, 'function':GRU_,'l1_reg': 0.0, 'H': 10, \n",
    "        'color': 'orange', 'label': 'GRU'},\n",
    "    'lstm': {\n",
    "        'model': None, 'function': LSTM_,'l1_reg': 0.0, 'H': 10, \n",
    "        'color':'red', 'label': 'LSTM'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['rnn']['history'].history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (25 mins 43 seconds)\n",
    "btc_hr_24 batch = 32 epoch = 100 1 min 13s\n",
    "btc_hr_1 batch = 32 epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = True # WARNING: Changing this to True will take many hours to run\n",
    "\n",
    "if cross_val:\n",
    "    n_units = [10, 20, 64]\n",
    "    l1_reg = [0, 0.001, 0.01, 0.1]\n",
    "    #l1_reg = [0]\n",
    "    \n",
    "    # A dictionary containing a list of values to be iterated through\n",
    "    # for each parameter of the model included in the search\n",
    "    param_grid = {'n_units': n_units, 'l1_reg': l1_reg}\n",
    "    \n",
    "    # In the kth split, TimeSeriesSplit returns first k folds \n",
    "    # as training set and the (k+1)th fold as test set.\n",
    "    tscv = TimeSeriesSplit(n_splits = 5)\n",
    "    \n",
    "    # A grid search is performed for each of the models, and the parameter set which\n",
    "    # performs best over all the cross-validation splits is saved in the `params` dictionary\n",
    "    for key in params.keys():\n",
    "        print('Performing cross-validation. Model:', key)\n",
    "        model = KerasRegressor(model=params[key]['function'], epochs=max_epochs, \n",
    "                               batch_size=batch_size, verbose=2, l1_reg=l1_reg, n_units=n_units)\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                            cv=tscv, n_jobs=1, verbose=2) # scikeras uses r^2 scoring by default\n",
    "        grid_result = grid.fit(x_train, y_train, callbacks=[es, tb])\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params_ = grid_result.cv_results_['params']\n",
    "        for mean, stdev, param_ in zip(means, stds, params_):\n",
    "            print(\"%f (%f) with %r\" % (mean, stdev, param_))\n",
    "            \n",
    "        params[key]['H'] = grid_result.best_params_['n_units']\n",
    "        params[key]['l1_reg']= grid_result.best_params_['l1_reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit model\n",
    "3 min 35.5 s\n",
    "\n",
    "25m 54.5s\n",
    "batch size = 32 \n",
    "epoch = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 min 35.5s\n",
    "for key in params.keys():\n",
    "    tf.random.set_seed(0)\n",
    "    print('Training', key, 'model')\n",
    "    # params[key]['H'] = best\n",
    "   # model = params[key]['function'](params[key]['H'], params[key]['l1_reg'])\n",
    "    model = params[key]['function'](64, 0)\n",
    "    params[key]['history'] = model.fit(x_train, y_train, epochs=max_epochs, \n",
    "                batch_size=batch_size, callbacks=[es, tb], shuffle=False, validation_data=(x_valid, y_valid))\n",
    "    params[key]['model'] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "2 min 54.9s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in params.keys():\n",
    "    model = params[key]['model']\n",
    "    model.summary()\n",
    "    \n",
    "    params[key]['pred_train'] = model.predict(x_train, verbose=1)\n",
    "    params[key]['MSE_train'] = mean_squared_error(y_train, params[key]['pred_train'])\n",
    "    \n",
    "    params[key]['pred_test'] = model.predict(x_test, verbose=1) \n",
    "    params[key]['MSE_test'] = mean_squared_error(y_test, params[key]['pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(compare):\n",
    "    max_points = 10**4\n",
    "    section = ['train', 'test']\n",
    "    l, u = (None, None) # lower and upper indices of range to plot \n",
    "    ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    x_vals = y_train_timestamps\n",
    "    for e in section:\n",
    "        for key in compare:\n",
    "            #print(f'pred_{e}')\n",
    "            y_vals = params[key][f'pred_{e}']\n",
    "            #print(f'({e} ' + 'MSE: %2e)')\n",
    "            label = params[key]['label'] + f' ({e}' + 'MSE: %.2e)' % params[key][f'MSE_{e}']\n",
    "            plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "        plt.plot(x_vals, y_test, c=\"black\", label=\"Observed\", lw=1)\n",
    "        start, end = x_vals.min(), x_vals.max()\n",
    "        xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "        xticks = xticks[::max(1, len(xticks)//30)]\n",
    "        for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "        plt.xticks(xticks, rotation=70)\n",
    "        plt.xlim(start, end)\n",
    "        plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "        plt.legend(loc=\"best\", fontsize=12)\n",
    "        plt.title('Observed vs Model Outputs (Testing)', fontsize=16)\n",
    "        plt.savefig(f'../figures/plot/{time_period}_{n_steps}_lag_{e}ing_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pts = 10**4\n",
    "compare = params.keys()\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable. \n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_train_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_train[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=70)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Training)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_training_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Testing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_test_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_test[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=70)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Testing)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_testing_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys()\n",
    "l, u = (None, None) # lower and upper indices of range to plot - e.g. (None, 10000)\n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_train_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds] - y_train[l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=80)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_training_performance_difference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys()\n",
    "l, u = (None, None) # e.g. (None, 100000) lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_test_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds] - y_test[l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=80)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Testing)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{dataset}/{start}_{end}_{n_steps}_lag_testing_performance_difference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"BTC-USD_hr_24\"\n",
    "df = pd.read_csv(f'../data/labeled_{dataset}.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index, unit='s')\n",
    "target = 'target_close'\n",
    "\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_features = ['last_low', 'last_high', \"last_open\", \"last_close\",\"last_volume\", \"current_open\", target] # continuous input\n",
    "n_features = len(use_features) - 1\n",
    "df = df[use_features]\n",
    "# already defined above\n",
    "#target = ['target_close'] # continuous output\n",
    "n_steps_ahead = 1 # forecasting horizon\n",
    "n_steps = 7\n",
    "#df = df['2021-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 1-t% training, t test\n",
    "def partition(df, t):\n",
    "    split_index = int(len(df) * (1-t))\n",
    "\n",
    "    df_train = df.iloc[:split_index]\n",
    "    df_test = df.iloc[split_index:]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(data, n_steps):\n",
    "    x = list()\n",
    "    y = list()\n",
    "    for i in range(len(data)):\n",
    "        window_end = i + n_steps\n",
    "        if window_end > len(data) - 1:\n",
    "            break\n",
    "        \n",
    "        x_window = data.iloc[i:window_end, :-1]\n",
    "        y_prediction = data.iloc[window_end - 1, -1]\n",
    "\n",
    "        x.append(x_window)\n",
    "        y.append(y_prediction)\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_X_train_full, multi_X_test = partition(df, 0.2)\n",
    "# train and validation set\n",
    "multi_X_train, multi_X_valid = partition(multi_X_train_full, 0.2)\n",
    "\n",
    "# save time stamps\n",
    "y_train_timestamps = multi_X_train.index[n_steps + n_steps_ahead - 1:]\n",
    "y_valid_timestamps = multi_X_valid.index[n_steps + n_steps_ahead - 1:]\n",
    "y_test_timestamps = multi_X_test.index[n_steps + n_steps_ahead - 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler()\n",
    "std_scaler.fit(multi_X_train_full)\n",
    "#print(f'Mean: {std_scaler.mean_}, Std Dev: {std_scaler.var_}')\n",
    "\n",
    "# use training mean and std dev to avoid future bias\n",
    "multi_X_train = std_scaler.transform(multi_X_train)\n",
    "multi_X_valid = std_scaler.transform(multi_X_valid)\n",
    "multi_X_test = std_scaler.transform(multi_X_test)\n",
    "\n",
    "# convert to df\n",
    "multi_X_train = pd.DataFrame(multi_X_train, columns=use_features)\n",
    "multi_X_valid = pd.DataFrame(multi_X_valid, columns=use_features)\n",
    "multi_X_test = pd.DataFrame(multi_X_test, columns=use_features)\n",
    "\n",
    "# train\n",
    "x_train, y_train = sliding_window(multi_X_train, n_steps)\n",
    "y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "\n",
    "# validation\n",
    "x_valid, y_valid = sliding_window(multi_X_valid, n_steps)\n",
    "y_valid = y_valid.reshape((y_valid.shape[0], 1))\n",
    "\n",
    "# test\n",
    "x_test, y_test = sliding_window(multi_X_test, n_steps)\n",
    "y_test = y_test.reshape((y_test.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Scaling (MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_scaler = preprocessing.MinMaxScaler()\n",
    "norm_scaler.fit(multi_X_train_full)\n",
    "#print(f'Mean: {norm_scaler.mean_}, Std Dev: {norm_scaler.var_}')\n",
    "\n",
    "# use training mean and std dev to avoid future bias\n",
    "multi_X_train = norm_scaler.transform(multi_X_train)\n",
    "multi_X_valid = norm_scaler.transform(multi_X_valid)\n",
    "multi_X_test = norm_scaler.transform(multi_X_test)\n",
    "\n",
    "# convert to df\n",
    "multi_X_train = pd.DataFrame(multi_X_train, columns=use_features)\n",
    "multi_X_valid = pd.DataFrame(multi_X_valid, columns=use_features)\n",
    "multi_X_test = pd.DataFrame(multi_X_test, columns=use_features)\n",
    "\n",
    "# train\n",
    "x_train, y_train = sliding_window(multi_X_train, n_steps)\n",
    "y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "\n",
    "# validation\n",
    "x_valid, y_valid = sliding_window(multi_X_valid, n_steps)\n",
    "y_valid = y_valid.reshape((y_valid.shape[0], 1))\n",
    "\n",
    "# test\n",
    "x_test, y_test = sliding_window(multi_X_test, n_steps)\n",
    "y_test = y_test.reshape((y_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Forecasting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_naive = np.roll(y_valid,1)\n",
    "# np.mean(keras.losses.mean_squared_error(y_valid, y_naive))\n",
    "naive_baseline(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Predictions (Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pts = 10**4\n",
    "compare = params.keys()\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable. \n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_train_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_train[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=70)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Training)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_training_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Testing Predictions (Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys() # e.g. ['rnn', 'alpharnn'] or ['lstm']\n",
    "l, u = (None, None) # lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_test_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "plt.plot(x_vals, y_test[l:u:ds], c=\"black\", label=\"Observed\", lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=70)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}$', rotation=0, fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Outputs (Testing)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_testing_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Error (Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys()\n",
    "l, u = (None, None) # lower and upper indices of range to plot - e.g. (None, 10000)\n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_train_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_train'][l:u:ds] - y_train[l:u:ds]\n",
    "    label = params[key]['label'] + ' (train MSE: %.2e)' % params[key]['MSE_train']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=80)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Training)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{time_period}_{n_steps}_lag_training_performance_difference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Testing Error (Multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = params.keys()\n",
    "l, u = (None, None) # e.g. (None, 100000) lower and upper indices of range to plot \n",
    "ds = max(1, len(y_train[l:u])//max_pts) # Downsampling ratio for under `max_pts`\n",
    "                                        # per series.  Set `None` to disable.\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "x_vals = y_test_timestamps[l:u:ds]\n",
    "for key in compare:\n",
    "    y_vals = params[key]['pred_test'][l:u:ds] - y_test[l:u:ds]\n",
    "    label = params[key]['label'] + ' (test MSE: %.2e)' % params[key]['MSE_test']\n",
    "    plt.plot(x_vals, y_vals, c=params[key]['color'], label=label, lw=1)\n",
    "start, end = x_vals.min(), x_vals.max()\n",
    "xticks =  [start.date() + timedelta(days=(1+i)) for i in range(1 + (end - start).days)]\n",
    "xticks = xticks[::max(1, len(xticks)//30)]\n",
    "plt.axhline(0, linewidth=0.8)\n",
    "for t in xticks: plt.axvline(x=t, c='gray', linewidth=0.5, zorder=0)\n",
    "plt.xticks(xticks, rotation=80)\n",
    "plt.xlim(start, end)\n",
    "plt.ylabel('$\\hat{Y}-Y$', fontsize=14)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.title('Observed vs Model Error (Testing)', fontsize=16)\n",
    "#plt.savefig(f'../figures/univariate/{dataset}/{start}_{end}_{n_steps}_lag_testing_performance_difference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe24fa82ef0276aa3bebf6833b77a4deb33ded5956560b7688b0e7890d905389"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('neural_network': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
